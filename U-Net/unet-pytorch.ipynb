{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "            # Encoder\n",
    "            xe11 = relu(self.e11(x))\n",
    "            xe12 = relu(self.e12(xe11))\n",
    "            xp1 = self.pool1(xe12)\n",
    "\n",
    "            xe21 = relu(self.e21(xp1))\n",
    "            xe22 = relu(self.e22(xe21))\n",
    "            xp2 = self.pool2(xe22)\n",
    "\n",
    "            xe31 = relu(self.e31(xp2))\n",
    "            xe32 = relu(self.e32(xe31))\n",
    "            xp3 = self.pool3(xe32)\n",
    "\n",
    "            xe41 = relu(self.e41(xp3))\n",
    "            xe42 = relu(self.e42(xe41))\n",
    "            xp4 = self.pool4(xe42)\n",
    "\n",
    "            xe51 = relu(self.e51(xp4))\n",
    "            xe52 = relu(self.e52(xe51))\n",
    "            \n",
    "            # Decoder\n",
    "            xu1 = self.upconv1(xe52)\n",
    "            print('xu1,', xu1.shape)\n",
    "            print('xe42,', xe42.shape)\n",
    "            xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "            xd11 = relu(self.d11(xu11))\n",
    "            xd12 = relu(self.d12(xd11))\n",
    "\n",
    "            xu2 = self.upconv2(xd12)\n",
    "            xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "            xd21 = relu(self.d21(xu22))\n",
    "            xd22 = relu(self.d22(xd21))\n",
    "\n",
    "            xu3 = self.upconv3(xd22)\n",
    "            xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "            xd31 = relu(self.d31(xu33))\n",
    "            xd32 = relu(self.d32(xd31))\n",
    "\n",
    "            xu4 = self.upconv4(xd32)\n",
    "            xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "            xd41 = relu(self.d41(xu44))\n",
    "            xd42 = relu(self.d42(xd41))\n",
    "\n",
    "            # Output layer\n",
    "            out = self.outconv(xd42)\n",
    "\n",
    "            return out\n",
    "\n",
    "# Define dataset and dataloader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_folder, self.images[idx])  # Assuming mask file names are the same as image file names\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        mask = Image.open(mask_name).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define data transformations\n",
    "data_transform = ToTensor()\n",
    "\n",
    "# Define data paths\n",
    "image_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/images\"\n",
    "mask_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/masks\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(image_folder, mask_folder, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize UNet model\n",
    "model = UNet(n_class=2)  # Set n_class=3 for three classes: background, class 1, class 2\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Assuming masks are of type LongTensor\n",
    "        masks = masks.squeeze(1)  # Remove the channel dimension (1)\n",
    "        \n",
    "        loss = criterion(outputs, masks.long())  # Ensure masks are of type long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=0) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=0) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=0) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=0) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=0) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=0) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=0) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=0) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=0) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "            # Encoder\n",
    "            xe11 = relu(self.e11(x))\n",
    "            xe12 = relu(self.e12(xe11))\n",
    "            xp1 = self.pool1(xe12)\n",
    "\n",
    "            xe21 = relu(self.e21(xp1))\n",
    "            xe22 = relu(self.e22(xe21))\n",
    "            xp2 = self.pool2(xe22)\n",
    "\n",
    "            xe31 = relu(self.e31(xp2))\n",
    "            xe32 = relu(self.e32(xe31))\n",
    "            xp3 = self.pool3(xe32)\n",
    "\n",
    "            xe41 = relu(self.e41(xp3))\n",
    "            xe42 = relu(self.e42(xe41))\n",
    "            xp4 = self.pool4(xe42)\n",
    "\n",
    "            xe51 = relu(self.e51(xp4))\n",
    "            xe52 = relu(self.e52(xe51))\n",
    "            \n",
    "            # Decoder\n",
    "            xu1 = self.upconv1(xe52)\n",
    "            print('xu1,', xu1.shape)\n",
    "            print('xe42,', xe42.shape)\n",
    "            xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "            xd11 = relu(self.d11(xu11))\n",
    "            xd12 = relu(self.d12(xd11))\n",
    "\n",
    "            xu2 = self.upconv2(xd12)\n",
    "            xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "            xd21 = relu(self.d21(xu22))\n",
    "            xd22 = relu(self.d22(xd21))\n",
    "\n",
    "            xu3 = self.upconv3(xd22)\n",
    "            xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "            xd31 = relu(self.d31(xu33))\n",
    "            xd32 = relu(self.d32(xd31))\n",
    "\n",
    "            xu4 = self.upconv4(xd32)\n",
    "            xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "            xd41 = relu(self.d41(xu44))\n",
    "            xd42 = relu(self.d42(xd41))\n",
    "\n",
    "            # Output layer\n",
    "            out = self.outconv(xd42)\n",
    "\n",
    "            return out\n",
    "\n",
    "# Define dataset and dataloader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_folder)\n",
    "        self.resize = transforms.Resize((572, 572))  # Resize images to 572x572\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_folder, self.images[idx])  # Assuming mask file names are the same as image file names\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        mask = Image.open(mask_name).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        # Resize image and mask\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define data transformations\n",
    "data_transform = ToTensor()\n",
    "\n",
    "# Define data paths\n",
    "image_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/images\"\n",
    "mask_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/masks\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(image_folder, mask_folder, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize UNet model\n",
    "model = UNet(n_class=2)  # Set n_class=3 for three classes: background, class 1, class 2\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Assuming masks are of type LongTensor\n",
    "        masks = masks.squeeze(1)  # Remove the channel dimension (1)\n",
    "        \n",
    "        loss = criterion(outputs, masks.long())  # Ensure masks are of type long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3\n",
    "Sudah bisa crop dari hasil encode untuk skip connnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=0) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=0) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=0) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=0) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=0) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=0) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=0) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=0) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=0) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "            # Encoder\n",
    "            xe11 = relu(self.e11(x))\n",
    "            xe12 = relu(self.e12(xe11))\n",
    "            xp1 = self.pool1(xe12)\n",
    "\n",
    "            xe21 = relu(self.e21(xp1))\n",
    "            xe22 = relu(self.e22(xe21))\n",
    "            xp2 = self.pool2(xe22)\n",
    "\n",
    "            xe31 = relu(self.e31(xp2))\n",
    "            xe32 = relu(self.e32(xe31))\n",
    "            xp3 = self.pool3(xe32)\n",
    "\n",
    "            xe41 = relu(self.e41(xp3))\n",
    "            xe42 = relu(self.e42(xe41))\n",
    "            xp4 = self.pool4(xe42)\n",
    "\n",
    "            xe51 = relu(self.e51(xp4))\n",
    "            xe52 = relu(self.e52(xe51))\n",
    "            \n",
    "            # Decoder\n",
    "            xu1 = self.upconv1(xe52)\n",
    "            crop_size = (xe42.size()[2] - xu1.size()[2]) // 2\n",
    "            xe42_crop = xe42[:, :, crop_size:crop_size + xu1.size()[2], crop_size:crop_size + xu1.size()[2]]\n",
    "            print('xu1,', xu1.shape)\n",
    "            print('xe42,', xe42_crop.shape)\n",
    "            xu11 = torch.cat([xu1, xe42_crop], dim=1)\n",
    "            xd11 = relu(self.d11(xu11))\n",
    "            xd12 = relu(self.d12(xd11))\n",
    "\n",
    "            xu2 = self.upconv2(xd12)\n",
    "            crop_size = (xe32.size()[2] - xu2.size()[2]) // 2\n",
    "            xe32_crop = xe32[:, :, crop_size:crop_size + xu2.size()[2], crop_size:crop_size + xu2.size()[2]]\n",
    "            xu22 = torch.cat([xu2, xe32_crop], dim=1)\n",
    "            xd21 = relu(self.d21(xu22))\n",
    "            xd22 = relu(self.d22(xd21))\n",
    "\n",
    "            xu3 = self.upconv3(xd22)\n",
    "            crop_size = (xe22.size()[2] - xu3.size()[2]) // 2\n",
    "            xe22_crop = xe22[:, :, crop_size:crop_size + xu3.size()[2], crop_size:crop_size + xu3.size()[2]]\n",
    "            xu33 = torch.cat([xu3, xe22_crop], dim=1)\n",
    "            xd31 = relu(self.d31(xu33))\n",
    "            xd32 = relu(self.d32(xd31))\n",
    "\n",
    "            xu4 = self.upconv4(xd32)\n",
    "            crop_size = (xe12.size()[2] - xu4.size()[2]) // 2\n",
    "            xe12_crop = xe12[:, :, crop_size:crop_size + xu4.size()[2], crop_size:crop_size + xu4.size()[2]]\n",
    "            xu44 = torch.cat([xu4, xe12_crop], dim=1)\n",
    "            xd41 = relu(self.d41(xu44))\n",
    "            xd42 = relu(self.d42(xd41))\n",
    "\n",
    "            # Output layer\n",
    "            out = self.outconv(xd42)\n",
    "\n",
    "            return out\n",
    "\n",
    "# Define dataset and dataloader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_folder)\n",
    "        self.resize = transforms.Resize((572, 572))  # Resize images to 572x572\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_folder, self.images[idx])  # Assuming mask file names are the same as image file names\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        mask = Image.open(mask_name).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        # Resize image and mask\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define data transformations\n",
    "data_transform = ToTensor()\n",
    "\n",
    "# Define data paths\n",
    "image_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/images\"\n",
    "mask_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/masks\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(image_folder, mask_folder, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize UNet model\n",
    "model = UNet(n_class=3)  # Set n_class=3 for three classes: background, class 1, class 2\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Assuming masks are of type LongTensor\n",
    "        masks = masks.squeeze(1)  # Remove the channel dimension (1)\n",
    "        \n",
    "        loss = criterion(outputs, masks.long())  # Ensure masks are of type long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=0) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=0) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=0) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=0) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=0) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=0) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=0) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=0) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=0) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=0)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=0)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=0)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=0)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=0)\n",
    "        \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xe42_resized = nn.functional.interpolate(xe42, size=xu1.size()[2:], mode='bilinear', align_corners=True)\n",
    "        print('xu1,', xu1.shape)\n",
    "        print('xe42,', xe42_resized.shape)\n",
    "        xu11 = torch.cat([xu1, xe42_resized], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xe32_resized = nn.functional.interpolate(xe32, size=xu2.size()[2:], mode='bilinear', align_corners=True)\n",
    "        print('xu2,', xu2.shape)\n",
    "        print('xe32,', xe32_resized.shape)\n",
    "        xu22 = torch.cat([xu2, xe32_resized], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xe22_resized = nn.functional.interpolate(xe22, size=xu3.size()[2:], mode='bilinear', align_corners=True)\n",
    "        print('xu3,', xu3.shape)\n",
    "        print('xe22,', xe22_resized.shape)\n",
    "        xu33 = torch.cat([xu3, xe22_resized], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xe12_resized = nn.functional.interpolate(xe12, size=xu4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        print('xu4,', xu4.shape)\n",
    "        print('xe12,', xe12_resized.shape)\n",
    "        xu44 = torch.cat([xu4, xe12_resized], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "        out_resized = nn.functional.interpolate(out, size=(572, 572), mode='bilinear', align_corners=True)\n",
    "\n",
    "        return out_resized\n",
    "\n",
    "# Define dataset and dataloader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_folder)\n",
    "        self.resize = transforms.Resize((572, 572))  # Resize images to 572x572\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_folder, self.images[idx])  # Assuming mask file names are the same as image file names\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        mask = Image.open(mask_name).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        # Resize image and mask\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define data transformations\n",
    "data_transform = ToTensor()\n",
    "\n",
    "# Define data paths\n",
    "image_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/images\"\n",
    "mask_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/masks\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(image_folder, mask_folder, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize UNet model\n",
    "model = UNet(n_class=3)  # Set n_class=3 for three classes: background, class 1, class 2\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch', epoch)\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Assuming masks are of type LongTensor\n",
    "        masks = masks.squeeze(1)  # Remove the channel dimension (1)\n",
    "        \n",
    "        loss = criterion(outputs, masks.long())  # Ensure masks are of type long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=0) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=0) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=0) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=0) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=0) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=0) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=0) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=0) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=0) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=0)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=0)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=0)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=0)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=0)\n",
    "        \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xe42_resized = nn.functional.interpolate(xe42, size=xu1.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu1,', xu1.shape)\n",
    "        # print('xe42,', xe42_resized.shape)\n",
    "        xu11 = torch.cat([xu1, xe42_resized], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xe32_resized = nn.functional.interpolate(xe32, size=xu2.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu2,', xu2.shape)\n",
    "        # print('xe32,', xe32_resized.shape)\n",
    "        xu22 = torch.cat([xu2, xe32_resized], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xe22_resized = nn.functional.interpolate(xe22, size=xu3.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu3,', xu3.shape)\n",
    "        # print('xe22,', xe22_resized.shape)\n",
    "        xu33 = torch.cat([xu3, xe22_resized], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xe12_resized = nn.functional.interpolate(xe12, size=xu4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu4,', xu4.shape)\n",
    "        # print('xe12,', xe12_resized.shape)\n",
    "        xu44 = torch.cat([xu4, xe12_resized], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "        out_resized = nn.functional.interpolate(out, size=(572, 572), mode='bilinear', align_corners=True)\n",
    "\n",
    "        return out_resized\n",
    "\n",
    "# Define dataset and dataloader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_folder)\n",
    "        self.resize = transforms.Resize((572, 572))  # Resize images to 572x572\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_folder, self.images[idx])  # Assuming mask file names are the same as image file names\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        mask = Image.open(mask_name).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        # Resize image and mask\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define data transformations\n",
    "data_transform = ToTensor()\n",
    "\n",
    "# Define data paths\n",
    "image_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/images\"\n",
    "mask_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/masks\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(image_folder, mask_folder, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize UNet model\n",
    "model = UNet(n_class=3)  # Set n_class=3 for three classes: background, class 1, class 2\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "best_loss = float('inf')  # Initialize best loss with infinity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    class_count = torch.zeros(3)  # Count of each class\n",
    "\n",
    "    print(\"epoch\", epoch)\n",
    "    for images, masks in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Assuming masks are of type LongTensor\n",
    "        masks = masks.squeeze(1)  # Remove the channel dimension (1)\n",
    "        \n",
    "        loss = criterion(outputs, masks.long())  # Ensure masks are of type long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate IOU\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Calculate intersection and union for each class\n",
    "        intersection = torch.zeros(3).to(predictions.device)\n",
    "        union = torch.zeros(3).to(predictions.device)\n",
    "\n",
    "        for i in range(3):\n",
    "            intersection[i] += ((predictions == i) & (masks == i)).sum().float()\n",
    "            union[i] += ((predictions == i) | (masks == i)).sum().float()\n",
    "\n",
    "        iou = (intersection + 1e-6) / (union + 1e-6)  # Add a small value to avoid division by zero\n",
    "        total_iou += iou.mean().item()\n",
    "\n",
    "    # Calculate average loss and IOU\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    average_iou = total_iou / len(dataloader)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}, IOU: {average_iou}\")\n",
    "\n",
    "    # Save the model if the current loss is better than the best loss\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "        # torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 6 with CUDA Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=0) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=0) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=0) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=0) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=0) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=0) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=0) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=0) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=0) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=0)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=0)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=0)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=0)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=0)\n",
    "        \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xe42_resized = nn.functional.interpolate(xe42, size=xu1.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu1,', xu1.shape)\n",
    "        # print('xe42,', xe42_resized.shape)\n",
    "        xu11 = torch.cat([xu1, xe42_resized], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xe32_resized = nn.functional.interpolate(xe32, size=xu2.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu2,', xu2.shape)\n",
    "        # print('xe32,', xe32_resized.shape)\n",
    "        xu22 = torch.cat([xu2, xe32_resized], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xe22_resized = nn.functional.interpolate(xe22, size=xu3.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu3,', xu3.shape)\n",
    "        # print('xe22,', xe22_resized.shape)\n",
    "        xu33 = torch.cat([xu3, xe22_resized], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xe12_resized = nn.functional.interpolate(xe12, size=xu4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu4,', xu4.shape)\n",
    "        # print('xe12,', xe12_resized.shape)\n",
    "        xu44 = torch.cat([xu4, xe12_resized], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "        out_resized = nn.functional.interpolate(out, size=(572, 572), mode='bilinear', align_corners=True)\n",
    "\n",
    "        return out_resized\n",
    "\n",
    "# Define dataset and dataloader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_folder)\n",
    "        self.resize = transforms.Resize((572, 572))  # Resize images to 572x572\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_folder, self.images[idx])  # Assuming mask file names are the same as image file names\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        mask = Image.open(mask_name).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        # Resize image and mask\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define data transformations\n",
    "data_transform = ToTensor()\n",
    "\n",
    "# Define data paths\n",
    "image_folder = r\"C:\\Users\\Admin\\Documents\\Naufal\\Program\\U-Net\\train\\images\"\n",
    "mask_folder = r\"C:\\Users\\Admin\\Documents\\Naufal\\Program\\U-Net\\train\\masks\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(image_folder, mask_folder, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize UNet model and move it to GPU\n",
    "# Move model to CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(n_class=3).to(device)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "best_loss = float('inf')  # Initialize best loss with infinity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    class_count = torch.zeros(3)  # Count of each class\n",
    "\n",
    "    print(\"epoch\", epoch)\n",
    "    for images, masks in dataloader:\n",
    "        # Move data to CUDA\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Assuming masks are of type LongTensor\n",
    "        masks = masks.squeeze(1)  # Remove the channel dimension (1)\n",
    "        \n",
    "        loss = criterion(outputs, masks.long())  # Ensure masks are of type long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate IOU\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Calculate intersection and union for each class\n",
    "        intersection = torch.zeros(3).to(predictions.device)\n",
    "        union = torch.zeros(3).to(predictions.device)\n",
    "\n",
    "        for i in range(3):\n",
    "            intersection[i] += ((predictions == i) & (masks == i)).sum().float()\n",
    "            union[i] += ((predictions == i) | (masks == i)).sum().float()\n",
    "\n",
    "        iou = (intersection + 1e-6) / (union + 1e-6)  # Add a small value to avoid division by zero\n",
    "        total_iou += iou.mean().item()\n",
    "\n",
    "    # Calculate average loss and IOU\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    average_iou = total_iou / len(dataloader)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}, IOU: {average_iou}\")\n",
    "\n",
    "    # Save the model if the current loss is better than the best loss\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "        # torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 7 Unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "255\n",
      "255\n",
      "255\n",
      "255\n",
      "torch.Size([572, 572])\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 196\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mprint\u001b[39m(masks\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    195\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Assuming masks are of type LongTensor\u001b[39;00m\n\u001b[1;32m    199\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Remove the channel dimension (1)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mcab/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mcab/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 118\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# print('xu4,', xu4.shape)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# print('xe12,', xe12_resized.shape)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m xu44 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xu4, xe12_resized], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m xd41 \u001b[38;5;241m=\u001b[39m relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md41\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxu44\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m xd42 \u001b[38;5;241m=\u001b[39m relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md42(xd41))\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Output layer\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mcab/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mcab/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mcab/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mcab/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=0) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=0) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=0) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=0) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=0) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=0) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=0) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=0) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=0) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=0) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=0)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=0)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=0)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=0)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=0)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=0)\n",
    "        \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xe42_resized = nn.functional.interpolate(xe42, size=xu1.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu1,', xu1.shape)\n",
    "        # print('xe42,', xe42_resized.shape)\n",
    "        xu11 = torch.cat([xu1, xe42_resized], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xe32_resized = nn.functional.interpolate(xe32, size=xu2.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu2,', xu2.shape)\n",
    "        # print('xe32,', xe32_resized.shape)\n",
    "        xu22 = torch.cat([xu2, xe32_resized], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xe22_resized = nn.functional.interpolate(xe22, size=xu3.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu3,', xu3.shape)\n",
    "        # print('xe22,', xe22_resized.shape)\n",
    "        xu33 = torch.cat([xu3, xe22_resized], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xe12_resized = nn.functional.interpolate(xe12, size=xu4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        # print('xu4,', xu4.shape)\n",
    "        # print('xe12,', xe12_resized.shape)\n",
    "        xu44 = torch.cat([xu4, xe12_resized], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "        out_resized = nn.functional.interpolate(out, size=(572, 572), mode='bilinear', align_corners=True)\n",
    "\n",
    "        return out_resized\n",
    "\n",
    "# Define dataset and dataloader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, mask_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_folder)\n",
    "        self.resize = transforms.Resize((572, 572))  # Resize images to 572x572\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_folder, self.images[idx])  # Assuming mask file names are the same as image file names\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        mask = Image.open(mask_name).convert(\"L\")  # Convert to grayscale\n",
    "        # print(np.max(mask))\n",
    "        # Resize image and mask\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define data transformations\n",
    "data_transform = ToTensor()\n",
    "\n",
    "# Define data paths\n",
    "image_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/images\"\n",
    "mask_folder = \"/Users/mohammadfaridnaufal/Library/CloudStorage/OneDrive-UniversitasSurabaya/S3/Project/Program/U-Net/train/images\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(image_folder, mask_folder, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize UNet model and move it to GPU\n",
    "# Move model to CUDA\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = UNet(n_class=3).to(device)\n",
    "model = UNet(n_class=3)  # Set n_class=3 for three classes: background, class 1, class 2\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "best_loss = float('inf')  # Initialize best loss with infinity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    class_count = torch.zeros(3)  # Count of each class\n",
    "\n",
    "    print(\"epoch\", epoch)\n",
    "    for images, masks in dataloader:\n",
    "        # Move data to CUDA\n",
    "        # images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Assuming masks are of type LongTensor\n",
    "        masks = masks.squeeze(1)  # Remove the channel dimension (1)\n",
    "        \n",
    "        loss = criterion(outputs, masks.long())  # Ensure masks are of type long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate IOU\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        print(predictions.shape)\n",
    "        print(\"min\", predictions.min().item())\n",
    "        print(\"max\", predictions.max().item())\n",
    "\n",
    "        # Calculate intersection and union for each class\n",
    "        intersection = torch.zeros(3).to(predictions.device)\n",
    "        union = torch.zeros(3).to(predictions.device)\n",
    "\n",
    "        for i in range(3):\n",
    "            intersection[i] += ((predictions == i) & (masks == i)).sum().float()\n",
    "            union[i] += ((predictions == i) | (masks == i)).sum().float()\n",
    "\n",
    "        iou = (intersection + 1e-6) / (union + 1e-6)  # Add a small value to avoid division by zero\n",
    "        total_iou += iou.mean().item()\n",
    "\n",
    "    # Calculate average loss and IOU\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    average_iou = total_iou / len(dataloader)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}, IOU: {average_iou}\")\n",
    "\n",
    "    # Save the model if the current loss is better than the best loss\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "        # torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
